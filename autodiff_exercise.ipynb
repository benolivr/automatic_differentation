{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation: From Scratch\n",
    "\n",
    "## Introduction\n",
    "As discussed in our presntation and the reading, calculating derivatives is central to machine learning. We generally categorize differentiation into four methods:\n",
    "\n",
    "1.  Manual Differentiation: Tedious and error-prone.\n",
    "2.  Symbolic Differentiation: Operates on mathematical expressions. Can lead to \"expression swell\" (exponentially large formulas).\n",
    "3.  Numerical Differentiation: Uses finite differences $\\left( \\dfrac{f(x+h) - f(x)}{h} \\right)$. Prone to truncation and round-off errors.\n",
    "4.  Automatic Differentiation (AD): Applies symbolic rules at the elementary operation level while keeping intermediate numerical results. \n",
    "\n",
    "In this notebook, we will implement symbolic and numeric differentiation ato see their problems, and then implement forward mode (using dual numbers) and reverse mode (by building a computational graph) to understand how deep learning frameworks compute gradients under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic Differentiation & Its Limitations\n",
    "\n",
    "Before diving into Automatic Differentiation (AD), it is crucial to understand why we don't simply use the methods taught in Calculus I (Symbolic Differentiation) or simple approximations (Numerical Differentiation).\n",
    "\n",
    "Symbolic differentiation transforms a mathematical expression into a new expression for the derivative using rules like the product rule: \n",
    "\n",
    "$$ \\frac{d}{dx}(f(x)g(x)) = f'(x)g(x) + f(x)g'(x) $$\n",
    "\n",
    "While exact, this method struggles with two main issues discussed in the course readings:\n",
    "1.  Expression Swell: The derivative expression can become exponentially larger than the original function.\n",
    "2.  Control Flow: Symbolic differentiation requires a closed-form expression. It fails (or becomes incredibly complex) when code involves `if` statements, loops, or recursion.\n",
    "\n",
    "#### Exercise 1: The Control Flow Problem\n",
    "Below is a Python function `strange_relu` that involves control flow. \n",
    "\n",
    "Your task: Manually implement its derivative, `symbolic_grad_strange_relu`. \n",
    "\n",
    "*Note how you must manually replicate the logic (the `if` statement) of the primal function to calculate the derivative correctly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strange_relu(x):\n",
    "    # A piecewise function that symbolic engines struggle to simplify generically\n",
    "    if x >= 2.0:\n",
    "        return x ** 3\n",
    "    elif x > 0.0:\n",
    "        return x ** 2\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def symbolic_grad_strange_relu(x):\n",
    "    \"\"\"\n",
    "    TODO: Manually implement the exact derivative of the function above.\n",
    "    You will need to manually mirror the control flow.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\t f(x)\t f'(x)\n",
      "----------------------\n",
      "-1.0\t 0.0\t 0.0\n",
      "1.0\t 1.0\t 2.0\n",
      "3.0\t 27.0\t 27.0\n",
      "Expected:\n",
      "-1.0\t 0.0\t 0.0\n",
      "1.0\t 1.0\t 2.0\n",
      "3.0\t 27.0\t 27.0\n"
     ]
    }
   ],
   "source": [
    "# Test your symbolic implementation\n",
    "test_points = [-1.0, 1.0, 3.0]\n",
    "print(\"x\\t f(x)\\t f'(x)\")\n",
    "print(\"----------------------\")\n",
    "for x in test_points:\n",
    "    print(f\"{x}\\t {strange_relu(x)}\\t {symbolic_grad_strange_relu(x)}\")\n",
    "\n",
    "print(\"\"\"Expected:\n",
    "-1.0\\t 0.0\\t 0.0\n",
    "1.0\\t 1.0\\t 2.0\n",
    "3.0\\t 27.0\\t 27.0\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Differentiation (Finite Differences)\n",
    "\n",
    "Numerical differentiation approximates the derivative using the definition of a limit, but with a fixed, small step size $h$. \n",
    "\n",
    "The Forward Difference is defined as:\n",
    "$$ f'(x) \\approx \\frac{f(x+h) - f(x)}{h} $$\n",
    "\n",
    "However, as noted in the PDF (Section 2.1, Figure 3), this method is prone to two types of errors:\n",
    "1.  Truncation Error: Mathematical error because $h \\neq 0$.\n",
    "2.  Round-off Error: Floating point issues when subtracting two nearly identical numbers ($f(x+h)$ and $f(x)$).\n",
    "\n",
    "We can reduce truncation error by using the Centered Difference:\n",
    "$$ f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h} $$\n",
    "\n",
    "#### Exercise 2: Comparing Finite Differences\n",
    "Your task: Implement both the forward and centered difference methods below and compare their accuracy against the exact derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diff(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def centered_diff(f, x, h=1e-5):\n",
    "    \"\"\"\n",
    "    TODO: Implement the centered difference formula.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Derivative at x=3.0: 27.0\n",
      "Forward Difference (h=0.0001): 27.000900 (Error: 9.00e-04)\n",
      "Centered Difference (h=0.0001): 27.000000 (Error: 1.01e-08)\n"
     ]
    }
   ],
   "source": [
    "x_val = 3.0\n",
    "h_val = 1e-4\n",
    "\n",
    "exact = symbolic_grad_strange_relu(x_val) \n",
    "fwd = forward_diff(strange_relu, x_val, h_val)\n",
    "cnt = centered_diff(strange_relu, x_val, h_val)\n",
    "\n",
    "print(f\"Exact Derivative at x={x_val}: {exact}\")\n",
    "print(f\"Forward Difference (h={h_val}): {fwd:.6f} (Error: {abs(exact-fwd):.2e})\")\n",
    "print(f\"Centered Difference (h={h_val}): {cnt:.6f} (Error: {abs(exact-cnt):.2e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Mode AD and Dual Numbers\n",
    "\n",
    "Forward mode AD can be implemented using dual numbers. A dual number is represented as $a + \\dot{a}\\epsilon$, where:\n",
    "*   $a$ is the primal (the value of the variable).\n",
    "*   $\\dot{a}$ is the tangent (the derivative of the variable).\n",
    "*   $\\epsilon$ is a \"nilpotent number,\" meaning $\\epsilon^2 = 0$.\n",
    "\n",
    "Dual number arithmetic mirrors differentiation, for example,\n",
    "1.  Addition: $(a + \\dot{a}\\epsilon) + (b + \\dot{b}\\epsilon) = (a+b) + (\\dot{a}+\\dot{b})\\epsilon$\n",
    "2.  Multiplication: $(a + \\dot{a}\\epsilon)(b + \\dot{b}\\epsilon) = ab + (a\\dot{b} + \\dot{a}b)\\epsilon$\n",
    "\n",
    "### Exercise 3: Implementing Dual Numbers\n",
    "\n",
    "Below is a class `DualNumber`. The addition and multiplication methods have been implemented for you. \n",
    "\n",
    "Your Task: Implement the `__pow__` method (for power $x^k$) and the `sin` function using the rules of dual numbers and differentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualNumber:\n",
    "    def __init__(self, primal, deriv=0.0):\n",
    "        self.primal = primal\n",
    "        self.deriv = deriv\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"DualNumber(Value={self.primal:.4f}, Derivative={self.deriv:.4f})\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def __add__(self, other):\n",
    "        # Handle adding a scalar constant\n",
    "        if isinstance(other, (int, float)):\n",
    "            return DualNumber(self.primal + other, self.deriv)\n",
    "        \n",
    "        # Handle adding two Dual Numbers\n",
    "        return DualNumber(self.primal + other.primal, self.deriv + other.deriv)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        # Handle subtracting a scalar constant\n",
    "        if isinstance(other, (int, float)):\n",
    "            return DualNumber(self.primal - other, self.deriv)\n",
    "        \n",
    "        # Handle subtracting two Dual Numbers\n",
    "        return DualNumber(self.primal - other.primal, self.deriv - other.deriv)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        # Handle dividing by scalar constant\n",
    "        if isinstance(other, (int, float)):\n",
    "            return DualNumber(self.primal / other, self.deriv / other)\n",
    "        \n",
    "        # Quotient Rule: (f'g - fg') / g^2 (Dividing two Dual Numbers)\n",
    "        new_primal = self.primal / other.primal\n",
    "        new_deriv = (self.deriv * other.primal - self.primal * other.deriv) / (other.primal ** 2)\n",
    "        return DualNumber(new_primal, new_deriv)\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        # Handle scalar constant divided by Dual Number\n",
    "        if isinstance(other, (int, float)):\n",
    "            new_primal = other / self.primal\n",
    "            new_deriv = (-other * self.deriv) / (self.primal ** 2)\n",
    "            return DualNumber(new_primal, new_deriv)\n",
    "        \n",
    "        raise NotImplementedError(\"Right division only implemented for scalar constants.\")\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # Handle multiplying by scalar constant\n",
    "        if isinstance(other, (int, float)):\n",
    "            return DualNumber(self.primal * other, self.deriv * other)\n",
    "        \n",
    "        # Product Rule: f(x)g'(x) + g(x)f'(x) (Multiplying two Dual Numbers)\n",
    "        new_primal = self.primal * other.primal\n",
    "        new_deriv = (self.primal * other.deriv) + (self.deriv * other.primal)\n",
    "        return DualNumber(new_primal, new_deriv)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        # Handle negation of Dual Number\n",
    "        return DualNumber(-self.primal, -self.deriv)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        \"\"\"\n",
    "        TODO: Implement the power rule for Dual Numbers.\n",
    "        Recall: d/dx (u(x)^n) = n * u(x)^(n-1) * u'(x)\n",
    "        Note: Assume 'power' is a scalar integer/float, not a DualNumber.\n",
    "        \"\"\"\n",
    "        \n",
    "        pass # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def dual_sin(x):\n",
    "    \"\"\"\n",
    "    TODO: Implement sine for a DualNumber.\n",
    "    Recall: d/dx sin(u(x)) = cos(u(x)) * u'(x)\n",
    "    \"\"\"\n",
    "    if isinstance(x, (int, float)):\n",
    "        return math.sin(x)\n",
    "    \n",
    "    pass # YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dual_log(x):\n",
    "    # Provided for the test function later\n",
    "    if isinstance(x, (int, float)):\n",
    "        return math.log(x)\n",
    "    return DualNumber(math.log(x.primal), x.deriv / x.primal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Jacobian with Forward Mode\n",
    "\n",
    "Forward mode computes the Jacobian one column at a time. \n",
    "\n",
    "If we have a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, we need $n$ passes to compute the full gradient/Jacobian. \n",
    "\n",
    "### Exercise 4: Compute the full gradient\n",
    "\n",
    "We'll use the same function from the reading: $$y= f(x_{1},x_{2}) = \\ln(x_{1})+x_{1}x_{2}-\\sin(x_{2})$$\n",
    "\n",
    "Your task: Using the `DualNumber` class above and the `test_function` below, write a script to compute the gradient $\\nabla f = \\left[\\dfrac{\\partial f}{\\partial x_1}, \\dfrac{\\partial f}{\\partial x_2}\\right]$ at $(2, 5)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: [5.5, 1.7163378145367738]\n",
      "Expected:   [5.5, 1.716...]\n"
     ]
    }
   ],
   "source": [
    "def test_function(x1, x2):\n",
    "    # f(x1, x2) = ln(x1) + x1*x2 - sin(x2)\n",
    "    return dual_log(x1) + (x1 * x2) - dual_sin(x2)\n",
    "\n",
    "# TODO: Calculate df/dx1 and df/dx2. \n",
    "# Hint: You need to initialize x1 and x2 similarly, but their dual number components should differ. Use the DualNumber class.\n",
    "\n",
    "# Pass 1: df/dx1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "grad_x1 = None  # Replace\n",
    "\n",
    "# Pass 2: df/dx2\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "grad_x2 = None  # Replace\n",
    "\n",
    "print(f\"Gradient: [{grad_x1}, {grad_x2}]\")\n",
    "print(\"Expected:   [5.5, 1.716...]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Mode AD (Graph-Based)\n",
    "\n",
    "Forward mode is efficient for $n \\ll m$ (few inputs, many outputs). However, in deep learning, we usually have a scalar loss function and millions of inputs (weights), so $n \\gg m$. \n",
    "\n",
    "For this, we use reverse mode AD, which builds a computational graph (Primal Trace) and then propagates derivatives backwards (Adjoint Trace). This allows us to compute the gradient of a scalar output with respect to *all* inputs in a single backward pass.\n",
    "\n",
    "### Exercise 5: Graph Construction\n",
    "We will define a `Value` class that stores:\n",
    "1. `data`: The numerical value.\n",
    "2. `grad`: The derivative of the final output with respect to this value (the adjoint $\\bar{v}$).\n",
    "3. `_backward`: A function that calculates the local gradients and sends them to the parents.\n",
    "\n",
    "This architecture allows us to define-by-run (dynamic computational graph), dealing with control flow naturally.\n",
    "\n",
    "Your task: implement the `relu` and `_backward` methods for our `Value` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data:.4f}, grad={self.grad:.4f})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            # Addition distributes gradient equally: d/dx (x+y) = 1\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            # Product rule: d/dx (xy) = y, d/dy (xy) = x\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        \"\"\"\n",
    "        TODO: Implement the ReLU (Rectified Linear Unit) operation.\n",
    "        Forward: x if x > 0 else 0\n",
    "        Backward: 1 * grad if x > 0 else 0\n",
    "        \n",
    "        This demonstrates AD's ability to handle control flow/logic.\n",
    "        \n",
    "        Hint: inside _backward, you need to update self.grad. \n",
    "        Recall that self.grad accumulates the gradients from its children. \n",
    "        For ReLU, if the original value was greater than 0, let the gradient (out.grad) pass through. \n",
    "        If it was 0 or less, \"kill\" the gradient (multiply by 0)\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Compute the forward value\n",
    "        out_data = None # YOUR CODE HERE\n",
    "        \n",
    "        # 2. Create the output Node\n",
    "        out = Value(out_data, (self,), 'ReLU')\n",
    "\n",
    "        # 3. Define the backward function closure\n",
    "        def _backward():\n",
    "            # YOUR CODE HERE\n",
    "            pass\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        # Topological sort to ensure we calculate gradients in correct order\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        \n",
    "        # Set gradient of the output (loss) to 1.0\n",
    "        self.grad = 1.0\n",
    "        # Apply chain rule in reverse order\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Case 1 (Positive) ---\n",
      "Output y:\t 5.0\t (Expected: 5.0)\n",
      "Grad a:  \t 3.0\t (Expected: 3.0)\n",
      "\n",
      "--- Case 2 (Negative) ---\n",
      "Output y:\t 0\t (Expected: 0.0)\n",
      "Grad a:  \t 0.0\t (Expected: 0.0)\n"
     ]
    }
   ],
   "source": [
    "# Case 1: Active ReLU\n",
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = Value(-1.0)\n",
    "\n",
    "z = a * b + c  # 2*3 - 1 = 5\n",
    "y = z.relu()   # ReLU(5) = 5\n",
    "\n",
    "y.backward()\n",
    "print(\"--- Case 1 (Positive) ---\")\n",
    "print(f\"Output y:\\t {y.data}\\t (Expected: 5.0)\")\n",
    "print(f\"Grad a:  \\t {a.grad}\\t (Expected: 3.0)\")\n",
    "\n",
    "# Case 2: Inactive ReLU\n",
    "a2 = Value(2.0)\n",
    "b2 = Value(-3.0)\n",
    "c2 = Value(-1.0)\n",
    "\n",
    "z2 = a2 * b2 + c2 # 2*-3 - 1 = -7\n",
    "y2 = z2.relu()    # ReLU(-7) = 0\n",
    "\n",
    "y2.backward()\n",
    "print(\"\\n--- Case 2 (Negative) ---\")\n",
    "print(f\"Output y:\\t {y2.data}\\t (Expected: 0.0)\")\n",
    "print(f\"Grad a:  \\t {a2.grad}\\t (Expected: 0.0)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci557 (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
